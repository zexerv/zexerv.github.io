<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>The Bellman Optimal Q-Value Function - My Personal Website</title>
  <link rel="stylesheet" href="../css/styles.css" />
</head>
<body>
  <header>
    <nav class="navbar">
      <div class="container">
        <a href="../index.html" class="logo">MySite</a>
        <ul class="nav-links">
          <li><a href="../index.html">Home</a></li>
          <li><a href="../about.html">About Me</a></li>
          <li><a href="../blog.html">Blog</a></li>
          <li><a href="../academia.html">Academia</a></li>
          <li><a href="../contact.html">Contact</a></li>
        </ul>
      </div>
    </nav>
  </header>

  <main>
    <div class="container">
      <article class="post-detail">
        <h1>The Bellman Optimal Q-Value Function</h1>
        <p class="post-date">Posted on February 25, 2025</p>
        <p>
          In reinforcement learning, the Bellman optimality equation forms the backbone of many learning algorithms.
          This post is dedicated to exploring the fundamentals of the Q-value function and how it helps an agent learn
          the best course of action over time. This content is currently a placeholder while I work on more detailed examples and explanations.
        </p>
        <pre>
Q*(s, a) = E [r + γ max(Q*(s', a')) | s, a]
        </pre>
        <p>
          Here, <code>s</code> and <code>a</code> represent the current state and action, <code>r</code> is the immediate reward,
          <code>s'</code> and <code>a'</code> denote the next state and possible actions, and <code>γ</code> is the discount factor.
          More in-depth analysis and case studies will follow in future updates.
        </p>
      </article>
    </div>
  </main>

  <footer>
    <div class="container">
      <p>&copy; 2025 My Personal Website. All rights reserved.</p>
    </div>
  </footer>
</body>
</html>
