<article>
    <h2>The Journey Begins</h2>
    <p>Welcome to my personal blog! This is where I'll share my thoughts, experiences, and insights.</p>
    <small>Posted on February 2, 2025</small>
</article>

<article>
    <h2>Why Learning Every Day Matters</h2>
    <p>Continuous learning is the key to growth. Whether it's reading, experimenting, or just thinking critically about the world, learning should never stop.</p>
    <small>Posted on February 8, 2025</small>
</article>

<article>
    <h2>My Thoughts on Robotics and AI</h2>
    <p>As I dive deeper into robotics, I realize how crucial AI is in improving automation and decision-making. Here are my latest thoughts.</p>
    <small>Posted on February 15, 2025</small>
</article>
<article>
    <h2>The Bellman Optimal Q-Value Function</h2>
    <p>
        In Reinforcement Learning, the <strong>Q-value function</strong> evaluates the best action to take at a given state. The Bellman optimality equation defines this recursively:
    </p>
    
    <p>
        The optimal Q-value function \( Q^*(s, a) \) is given by:
    </p>

    <pre>
Q*(s, a) = E [r + γ max(Q*(s', a')) | s, a]
    </pre>

    <p>Where:</p>
    <ul>
        <li>\( s, a \) → Current state and action.</li>
        <li>\( s', a' \) → Next state and possible next action.</li>
        <li>\( r \) → Immediate reward.</li>
        <li>\( \gamma \) → Discount factor for future rewards.</li>
        <li>\( \max_{a'} Q^*(s', a') \) → The highest expected reward for the next state.</li>
    </ul>

    <p>
        This equation is fundamental in Q-learning and Deep Q Networks (DQNs), allowing an agent to learn the best policy over time.
    </p>

    <small>Posted on February 25, 2025</small>
</article>