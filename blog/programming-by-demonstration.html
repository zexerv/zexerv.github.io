<!DOCTYPE html>
<html lang="en" data-theme="dark">
<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Programming by Demonstration Deep Dive - Mo Saeidi</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;500;700&family=Roboto+Mono:wght@400;500&display=swap" rel="stylesheet">

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css"/>

    <link rel="stylesheet" href="../assets/css/base.css" />
    <link rel="stylesheet" href="../assets/css/layout.css" />
    <link rel="stylesheet" href="../assets/css/components/navbar.css" />
    <link rel="stylesheet" href="../assets/css/components/buttons.css" />
    <link rel="stylesheet" href="../assets/css/components/footer.css" />
    <link rel="stylesheet" href="../assets/css/pages/post.css" />

</head>
<body>
    <div class="page-wrapper">
        <header class="header">
             <nav class="navbar">
                <div class="container">
                    <button id="mobile-menu-btn" class="mobile-menu-btn" aria-label="Toggle Menu" aria-expanded="false">
                        <span class="bar"></span>
                        <span class="bar"></span>
                        <span class="bar"></span>
                    </button>
                    <ul class="nav-links">
                        <li><a href="../index.html">MoSa</a></li> <li><a href="../projects.html">Projects</a></li>
                        <li><a href="../blog.html">Blog</a></li>
                        <li><a href="../academia.html">Academia</a></li>
                    </ul>
                </div>
            </nav>
        </header>

        <main class="main-content">
            <div class="container post-container">
                 <article>
                    <header class="post-header">
                        <div class="post-meta">
                            <time datetime="2025-04-10">April 10, 2025</time> </div>
                        <h1 class="post-title">Teaching Robots Like We Teach People: A Deep Dive into Programming by Demonstration</h1>
                    </header>

                    <section class="post-content">
                        <p>We humans possess an incredible, almost effortless ability to learn by watching. From a child mimicking a parent's wave to an apprentice observing a master craftsman, learning through demonstration is fundamental to how we acquire skills. Yet, for decades, programming our most advanced machines – robots – has often felt like the antithesis of this intuitive process. We painstakingly define waypoints, write explicit conditional logic, and debug complex code to achieve tasks that a human might grasp after just a few observations.</p>

                        <p>This disconnect has always fascinated me, driving my journey from the deterministic world of mechanical engineering and classical control (shoutout to KNTU labs!) into the more nuanced domains of automation, machine learning, and robotics. My current thesis work here at Politecnico di Milano, within the MERLIN Laboratory's ARTO project, is centered precisely on bridging this gap. We're exploring how Programming by Demonstration (PbD) can empower even non-programmers to teach complex tasks – like manipulating delicate cockpit interfaces – to industrial robots like the UR5e. But as I delve deeper, it's clear that PbD is far more intricate than simply hitting "record" and "playback." It’s a fascinating confluence of perception, learning, representation, control, and even a bit of understanding <em>intent</em>.</p>

                        <h2>What Exactly is Programming by Demonstration?</h2>

                        <p>At its core, PbD (also known as Learning from Demonstration - LfD, or Imitation Learning) is a paradigm where a robot learns a new skill by observing demonstrations performed by a teacher, typically a human. Instead of relying solely on explicit programming instructions, the robot extracts the essential characteristics of the task from the demonstrated examples.</p>

                        <p>The allure is obvious. Imagine factory workers quickly re-tasking robots without needing specialized programmers, surgeons transferring intricate techniques to robotic assistants, or personal robots learning household chores simply by watching us. Compared to traditional methods, PbD offers:</p>
                        <ul>
                            <li><strong>Intuition:</strong> It leverages our natural teaching and learning mechanisms.</li>
                            <li><strong>Speed for Complexity:</strong> Demonstrating a complex trajectory (like smoothly maneuvering around obstacles or manipulating a series of controls) can be much faster than coding it.</li>
                            <li><strong>Accessibility:</strong> It potentially lowers the barrier for robot programming, democratizing automation.</li>
                        </ul>
                        <p>However, it's not a silver bullet. Demonstrations can be noisy, suboptimal, or ambiguous. Simply mimicking a single demonstration rarely leads to a robust or adaptable skill. The real magic lies in how we move from observation to generalized, reliable execution.</p>

                        <h2>Capturing the Act: The Sensorimotor Interface</h2>

                        <p>The first step is capturing the demonstration itself. How the robot "sees" or "feels" the demonstration profoundly impacts what it can learn. Common methods include:</p>
                        <ol>
                            <li><strong>Kinesthetic Teaching:</strong> Physically guiding the robot's end-effector through the desired motion. This provides direct joint-level or Cartesian-level data but requires physical access and can be cumbersome for highly dynamic or forceful tasks. The robot directly feels the intended path.</li>
                            <li><strong>Teleoperation:</strong> Using an input device (joysticks, haptic masters, VR controllers) to remotely control the robot during the demonstration. This is the approach we're using in ARTO, capturing human intent via joystick inputs. It keeps the human safe but introduces challenges in mapping the potentially lower-dimensional input device commands to the robot's high-dimensional state space and dynamics. The operator's own control limitations also become part of the data.</li>
                            <li><strong>Vision-Based Observation:</strong> Using cameras (external or robot-mounted) to track human motion (body pose, hand movements) or the movement of objects being manipulated. This is perhaps the most "human-like" method but suffers from occlusion, perspective issues, and the significant challenge of the <em>correspondence problem</em> – mapping observed human actions onto the robot's potentially different kinematic structure (e.g., my work on hand pose estimation touched upon related challenges).</li>
                            <li><strong>Other Sensors:</strong> Force/torque sensors can capture interaction forces, crucial for tasks involving contact. Tactile sensors can provide information about grip and surface properties.</li>
                        </ol>
                        <p>A critical choice here is <em>what</em> data to record. Is it just end-effector position? Orientation? Joint angles? Velocities? Applied forces? Contextual information from the environment? The richer the data, the more potential for learning, but also the higher the complexity. In ARTO, we focus on joystick inputs representing desired velocity, but translating this effectively requires careful consideration of the robot's dynamics and the task constraints.</p>

                        <h2>From Raw Data to Robot Skill: The Learning Challenge</h2>

                        <p>Once we have demonstration data (often time series of poses, velocities, etc.), the core learning problem begins: how do we transform these specific examples into a generalized representation of the skill?</p>

                        <h3>Trajectory Representation</h3>
                        <p>Simply replaying recorded points is brittle. We need a more flexible representation.</p>
                        <ul>
                            <li><em>Dynamic Movement Primitives (DMPs):</em> A popular technique representing trajectories as systems of differential equations with attractor dynamics. They offer excellent properties for temporal scaling (speeding up/slowing down) and spatial scaling (adapting to start/goal variations), and can incorporate feedback terms for obstacle avoidance.</li>
                            <li><em>Probabilistic Methods (GMM/GMR):</em> Gaussian Mixture Models (GMMs) can capture the statistical distribution of multiple demonstrations, representing variations and correlations between different dimensions (e.g., position and orientation). Gaussian Mixture Regression (GMR) can then extract a smooth, generalized trajectory from the GMM.</li>
                            <li><em>Neural Network Policies:</em> End-to-end approaches using recurrent networks (LSTMs, Transformers) or convolutional networks (for visual input) can directly map states (or observations) to actions, learning the trajectory implicitly. Goal-conditioned policies allow specifying different goals at execution time.</li>
                        </ul>

                        <h3>Generalization</h3>
                        <p>This is arguably the most crucial part. The robot needs to perform the task correctly even if the starting position, object location, or environmental context changes slightly. The chosen trajectory representation heavily influences generalization capabilities (e.g., DMPs scale well, GMM/GMR captures variance). Techniques often involve identifying key frames or phases in the demonstration, learning coordinate transformations to adapt to new situations, or using goal-conditioned learning approaches.</p>

                        <h3>Inferring Intent and Constraints</h3>
                        <p>A demonstration rarely encodes <em>just</em> the path; it implicitly encodes the <em>objective</em> and the <em>constraints</em>. Why did the human move <em>that</em> way? Was it to avoid an obstacle? Maintain a specific tool orientation? Apply a certain force? Keep a cup upright?</p>
                        <ul>
                            <li><em>Inverse Reinforcement Learning (IRL):</em> Instead of learning the policy directly, IRL tries to infer the underlying <em>reward function</em> the demonstrator was implicitly optimizing. This inferred reward function can then be used to train a more optimal policy via RL.</li>
                            <li><em>Learning Cost Functions:</em> Relatedly, we can frame the problem as learning a cost function that explains the demonstrated trajectory. In ARTO, for instance, developing specialized cost functions within sampling-based planners (like RRT algorithms) helps interpret the human joystick inputs not just as velocity commands, but as expressions of intent navigating towards a goal while respecting implicit constraints. This links demonstration interpretation directly to the planning/optimization phase.</li>
                        </ul>

                        <h2>Executing the Learned Skill: The Realm of Control & Optimization</h2>

                        <p>A learned trajectory or policy is just the start. Executing it reliably and safely on a physical robot brings us back firmly into the domain of control theory and optimization.</p>

                        <h3>Trajectory Planning & Refinement</h3>
                        <p>The generalized trajectory from the learning step might not be dynamically feasible, smooth, or optimal for the robot.</p>
                        <ul>
                            <li><em>Motion Planners (OMPL/MoveIt):</em> As we utilize in ARTO, libraries like OMPL within frameworks like MoveIt are essential. The learned trajectory can serve as a reference or bias for planners like RRT* or optimization-based planners (like STOMP or CHOMP) to find a feasible path that respects joint limits, collision constraints, and potentially optimizes for criteria like path length or smoothness. The demonstration effectively prunes the search space.</li>
                            <li><em>Trajectory Optimization:</em> Techniques like Differential Dynamic Programming (DDP) or trajectory optimization solvers (using tools like CasADi) can take a nominal trajectory (perhaps from PbD) and refine it to minimize control effort, execution time, or other costs while strictly enforcing constraints.</li>
                        </ul>

                        <h3>Constraint Handling</h3>
                        <p>This is non-negotiable in the real world.</p>
                        <ul>
                            <li><em>Kinematic Constraints:</em> Joint limits, velocity/acceleration limits must be respected.</li>
                            <li><em>Environmental Constraints:</em> Real-time obstacle avoidance needs to integrate with the learned plan, often requiring reactive control layers.</li>
                            <li><em>Task Constraints:</em> Maintaining tool orientation, applying specific forces (requiring impedance or admittance control), or following specific path segments.</li>
                            <li><em>Singularity Avoidance:</em> A key challenge, especially for manipulators, as highlighted in the ARTO project goals. The learned policy or the execution controller must actively avoid configurations where the robot loses dexterity or experiences high joint velocities. This often involves monitoring manipulability measures and incorporating avoidance strategies directly into the planning, optimization, or feedback control layers. My background in control systems, dealing with actuation constraints and stability, feels particularly relevant here.</li>
                        </ul>

                        <h3>Feedback Control</h3>
                        <p>Executing a pre-computed trajectory open-loop is rarely sufficient. Sensors provide feedback about the actual robot state and the environment, which must be used to correct deviations.</p>
                        <ul>
                            <li><em>Tracking Controllers:</em> PID controllers, computed torque control, or more advanced nonlinear controllers ensure the robot follows the desired trajectory accurately.</li>
                            <li><em>Impedance/Force Control:</em> Essential for tasks involving contact, allowing the robot to comply with environmental forces rather than rigidly resisting them. Learned policies might specify desired interaction forces or stiffness profiles.</li>
                            <li><em>Reactive Layers:</em> Obstacle avoidance reflexes or safety stops layered on top of the primary learned/planned motion.</li>
                        </ul>

                        <h2>The Unfolding Landscape: Challenges and Open Questions</h2>

                        <p>Despite the progress, PbD is an active research area with many fascinating open problems:</p>
                        <ul>
                            <li><strong>Data Efficiency:</strong> How can robots learn complex skills from just one or a handful of demonstrations, mimicking human efficiency?</li>
                            <li><strong>Learning Structure & Logic:</strong> Moving beyond single trajectories to tasks with branching logic, conditional execution, or sequences of sub-tasks.</li>
                            <li><strong>Interpreting Ambiguity & Suboptimality:</strong> Human demonstrations are rarely perfect. How can the robot discern the true goal from noisy or suboptimal movements? How to effectively fuse information from multiple, potentially conflicting, demonstrations?</li>
                            <li><strong>Safety & Verification:</strong> Critically important: How can we provide guarantees on the safety and reliability of policies learned from demonstration, especially when they generalize to unseen situations?</li>
                            <li><strong>Lifelong Adaptation:</strong> How can a robot initially learn from demonstration and then continue to refine its skills through practice and self-exploration (RL) in a safe and efficient manner?</li>
                        </ul>

                        <h2>Personal Reflections & Future Outlook</h2>

                        <p>Working on PbD feels like operating at a unique nexus. It forces us to consider not just the robot's dynamics and control (my traditional background), but also the nuances of human motor control, intention, and communication. How do we build interfaces that allow humans to express their intent effectively? How do we design learning algorithms that capture the essence of a skill, not just the superficial movements? And how do we ensure the resulting robot behavior is not only effective but also safe and predictable?</p>

                        <p>It connects back to the idea of exploration, too. PbD provides a strong inductive bias – the robot starts with a good idea of how to perform the task. But perhaps true mastery requires the robot to explore <em>around</em> the demonstration, to discover variations or optimizations the human didn't show. Balancing the efficiency of imitation with the performance potential of exploration remains a key theme.</p>

                        <p>The dream isn't just about making programming easier; it's about creating a more natural, collaborative relationship between humans and robots. It's about building machines that can learn from us in ways that feel intuitive, enabling automation to augment human capabilities across a wider range of domains.</p>

                        <h2>Conclusion: A Synergy in Motion</h2>

                        <p>Programming by Demonstration is more than a collection of algorithms; it's a shift in perspective. It acknowledges the richness of information conveyed through action and seeks to translate that into robotic competence. It requires a deep appreciation for both the intricacies of machine learning and the established principles of control theory and optimization. Neither field alone holds all the answers. The path forward, as I see it from my vantage point at the MERLIN lab, lies in their synergistic combination – using demonstrations to bootstrap learning, control theory to ensure safety and robustness, and optimization to achieve efficiency. The journey to truly intuitive robot teaching is ongoing, but it's one of the most compelling explorations in modern robotics.</p>

                    </section>
                 </article>

                 <a href="../blog.html" class="back-to-blog"><i class="fas fa-arrow-left"></i> Back to Blog List</a> </div> </main>

        <footer class="footer">
            <div class="container">
                <p>&copy; <span id="current-year"></span> Mo Saeidi. All rights reserved.</p>
            </div>
        </footer>
    </div> <script src="../assets/js/main.js"></script>
</body>
</html>